{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b55d4831-92cc-4a18-b043-ac8a95592d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from utils.helper import fn_plot_torch_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a97a8e4-f3e0-47e3-adf2-3b3198b7fbc5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m RANDOM_STATE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m24\u001b[39m\n\u001b[0;32m     13\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mRandomState(seed \u001b[38;5;241m=\u001b[39m RANDOM_STATE)\n\u001b[1;32m---> 14\u001b[0m \u001b[43mtf\u001b[49m\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mset_seed(RANDOM_STATE) \u001b[38;5;66;03m# set_seed in tf\u001b[39;00m\n\u001b[0;32m     15\u001b[0m rng\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mdefault_rng(seed\u001b[38;5;241m=\u001b[39m RANDOM_STATE)\n\u001b[0;32m     17\u001b[0m ALPHA \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m.1\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "###------------------------------\n",
    "### global variables\n",
    "##-----------------\n",
    "RANDOM_STATE = 24\n",
    "# np.random.RandomState(seed = RANDOM_STATE)\n",
    "# nn.random.set_seed(RANDOM_STATE)\n",
    "# rng= np.random.default_rng(seed= RANDOM_STATE)\n",
    "\n",
    "N_SAMPLE  = 1000\n",
    "NOISE = .2\n",
    "ALPHA = .001\n",
    "TEST_SIZE = .2\n",
    "EPOCHS = 2000 #NO. of itersation used to optimize weights\n",
    "# input and output directories\n",
    "\n",
    "params = {'legend.fontsize' : 'medium', \n",
    "          'figure.figsize' : (15, 4),\n",
    "          'axes.labelsize' : 'medium',\n",
    "          'axes.titlesize' : 'large',\n",
    "          'xtick.labelsize' : 'medium',\n",
    "          'ytick.labelsize' : 'medium',\n",
    "         }\n",
    "loss_hist = {} # creating an empty dictionary which will store loss and epoch later so that we can make loss curve later\n",
    "\n",
    "plt.rcParams.update(params)\n",
    "\n",
    "CMAP = plt.cm.coolwarm\n",
    "\n",
    "# plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f09f2d6-74ab-441f-9c4d-97c3f3243d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Machine specific code\n",
    "\n",
    "# physical_device = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "# if len(physical_device) >0:\n",
    "    # tf.config.experimental.set_memory_growth(physical_device[0],True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058b2fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.make_moons(n_samples = N_SAMPLE, # n_samples: how many datasets in it\n",
    "                          noise = NOISE, # peportion of randommness added to data so that it matched to real life data , without noise it becomes perfect half circle in moon dataset\n",
    "                          shuffle = True, \n",
    "                          random_state = RANDOM_STATE)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d727602b-e925-48d6-9536-d533d544dad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test=   train_test_split(X,y , random_state= RANDOM_STATE, stratify=y, test_size= TEST_SIZE)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09159bbe-249f-4708-aed3-5758f4762682",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5d4483-ef7c-47ec-9e09-dd9b4c629032",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device # manually set device in pytorch "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01edaf5-db7e-4861-98a6-ec6087748b0e",
   "metadata": {},
   "source": [
    "## making model using pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39661eaf-6ee7-4245-a1f4-cedfb3430658",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "# input is not passsed as list here\n",
    "model = nn.Sequential(  ## nn stand for neural network torch.nn\n",
    "\n",
    "    nn.Linear(input_dim, 5), # no of neurons=5\n",
    "    nn.ReLU(), # activation function\n",
    "    nn.Linear(5, 5), # no of neurons=5, no. of input 5\n",
    "    nn.ReLU(), \n",
    "    nn.Linear(5, 4), # no of neurons=4, no of input 5\n",
    "    nn.ReLU(), \n",
    "    nn.Linear(4, 3), \n",
    "    nn.ReLU(), \n",
    "    nn.Linear(3, 2)\n",
    "    ).to(device = device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adff5879-b8e2-4afc-af8c-a3d5afe26584",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144c567a-4dd2-4556-8c28-425c29451467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have to convert datatype from numpy to tensor in torch\n",
    "# moving all the training data to device\n",
    "train_X = torch.tensor(X_train, dtype = torch.float32, device = device) #float32 is the min data precision which can give us good result, tensor flow automatically\n",
    "# device= device means we are putting it to the available device\n",
    "train_y = torch.tensor(y_train, dtype = torch.int64, device = device) \n",
    "\n",
    "\n",
    "test_X = torch.tensor(X_test, dtype = torch.float32, device = device)\n",
    "test_y = torch.tensor(y_test, dtype = torch.int64, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e286a81-5660-48df-bce1-8f54118425a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4dab9c-56ab-4c38-84a9-be43fa5bb7a1",
   "metadata": {},
   "source": [
    "`optimizer.zero_grad()` clears the gradients of all the parameters that the optimizer is managing. Gradients are accumulated in PyTorch by default during the backward pass (when you call loss.backward()), so this step ensures that the gradients donâ€™t mix between different training iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6859a6-8eac-4bee-9e28-10130c559a0a",
   "metadata": {},
   "source": [
    "`optimizer.step()` uses the gradients that were calculated during the backward pass `(loss.backward())` to adjust the parameters of the model.\n",
    "The adjustments are based on the optimization algorithm (e.g., SGD, Adam, etc.) and its hyperparameters (e.g., learning rate, momentum)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e190f7-ae54-461f-be90-f4406e98197c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### defining loss function # just  like compile of tensorflow\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "#Adam needs two parameters over each weight\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = ALPHA)# lr is learning rate\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train() #model.train() means that we are using model in train mode thoufh if use model directly then default is also train\n",
    "    # we have to do this because wiegrhs gets updated onlu in training mode\n",
    "    predict_proba = model(train_X)\n",
    "    curr_loss = loss_fn(predict_proba, train_y)\n",
    "    \n",
    "    #Backpropagation , just like fit part of tensorflow\n",
    "    optimizer.zero_grad() # because tensors are immutable we can't change them  # it resets the values in the GPU as tensors are immutable\n",
    "    curr_loss.backward()\n",
    "    optimizer.step() # according to optimizer , move one step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f531b3-c368-4f2e-b318-1071d1ee7653",
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854b47d8-8a48-412e-b367-1a2989f22d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(train_X)\n",
    "y_pred = torch.argmax(output, dim = 1).cpu().numpy()#dim is like axis in dataframe, .cpu()  moves to cpu, converted to numpy\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f6ab90-5815-4147-b62b-88045adb128a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(train_y.cpu().numpy(), y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63e467d-434f-492e-b9ab-106555d94b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "model = nn.Sequential(  ## nn stand for neural network torch.nn\n",
    "\n",
    "    nn.Linear(input_dim, 5), # no of neurons=5\n",
    "    nn.ReLU(), # activation function\n",
    "    nn.Linear(5, 5), # no of neurons=5, no. of input 5\n",
    "    nn.ReLU(), \n",
    "    nn.Linear(5, 4), # no of neurons=4, no of input 5\n",
    "    nn.ReLU(), \n",
    "    nn.Linear(4, 3), \n",
    "    nn.ReLU(), \n",
    "    nn.Linear(3, 2)\n",
    "    ).to(device = device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9b43ac-0fbc-4089-a581-1eefe6a9b877",
   "metadata": {},
   "outputs": [],
   "source": [
    "### defining loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "#Adam needs two parameters over each weight\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = ALPHA)# lr is learning rate\n",
    "\n",
    "#list to collect the progress\n",
    "loss =[]\n",
    "tloss = []\n",
    "n_epoch = []\n",
    "acc = []\n",
    "tacc = [] # testing accuracy\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train() #model.train() means that we are using model in train mode thoufh if use model directly then default is also train\n",
    "    # we have to do this because wiegrhs gets updated onlu in training mode\n",
    "    predict_proba = model(train_X)\n",
    "    curr_loss = loss_fn(predict_proba, train_y)\n",
    "    \n",
    "    #Backpropagation , just like fit part of tensorflow\n",
    "    optimizer.zero_grad() # because tensors are immutable we can't change them  # it resets the values in the GPU as tensors are immutable\n",
    "    curr_loss.backward()\n",
    "    optimizer.step() # according to optimizer , move one step\n",
    "\n",
    "    loss.append(curr_loss.data.item())\n",
    "\n",
    "    y_pred = torch.argmax(predict_proba, 1).cpu().numpy() \n",
    "\n",
    "    curr_acc = accuracy_score(train_y.cpu().numpy(), y_pred)\n",
    "\n",
    "    acc.append(curr_acc)\n",
    "\n",
    "    model.eval() \n",
    "    test_proba = model(test_X)\n",
    "    test_loss = loss_fn(test_proba, test_y)\n",
    "    tloss.append(test_loss.data.item())\n",
    "\n",
    "    y_pred = torch.argmax(test_proba, 1) \n",
    "    test_acc = accuracy_score(test_y.cpu().numpy(), y_pred)\n",
    "    tacc.append(test_acc)\n",
    "\n",
    "    n_epoch.append(epoch)\n",
    "\n",
    "    if epoch % 500 == 0:\n",
    "        print(f'Epoch: {epoch:>5d} |Loss: {curr_loss:.5f}/{test_loss:.5f} | ACC: {curr_acc: .5f}/{test_acc: .5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f942d909-c2a0-4658-a167-9f9923ef73bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(n_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d40931-100d-4cc0-ba49-59b8c19223ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hist_df = pd.DataFrame({'epoch': n_epoch,\n",
    "                        'loss': loss,\n",
    "                        'test_loss' : tloss,\n",
    "                        'acc' : acc,\n",
    "                        'test_Acc': tacc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b963587b-1a0e-42fb-a589-e29cd447c4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1d5dc1-d6c4-42bf-a4e6-403045a9594c",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(hist_df.head())\n",
    "fig, ax = plt.subplots(1,2)\n",
    "\n",
    "# the required plot\n",
    "hist_df.plot(y= ['loss','test_loss'], ax= ax[0])\n",
    "hist_df.plot(y= ['acc','test_Acc'], ax= ax[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b98555-843e-4ce9-9610-f68da1336fe6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7e4f3d-2a70-4a59-a656-1b2604e732ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15273b25-6a1e-4ad2-a1ed-a9ea95c1dc2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00890b68-8844-4f9d-96b7-5f3a4709068a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function\n",
    "#criterias: on what we are defining loss , \n",
    "#sparse means: input is column vector, \n",
    "#check y and y_pred same or not\n",
    "# from_logits = True means i did not put sigmoid or softmax at the last layer, google will automatically handle it.\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af051b96-e640-40a3-b62b-faaa8542b892",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn(y_train[:1], prediction).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87774739-f877-40a1-804b-78381d730ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for classification : metrics = accuracy\n",
    "model.compile(optimizer = 'adam', loss= loss_fn, metrics= ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5a6eea-9d5a-4f46-b300-20c83dda863d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model fitting \n",
    "# supply x, y and validation data , at each epoch check how model is doing on validation\n",
    "\n",
    "history =  model.fit(x= X_train, y=y_train, validation_data= [X_test, y_test], epochs= EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa50818-51b4-4b41-a40e-9bc508bfbd02",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f68b5f-3821-4363-a65d-48afa9684005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep eye on loss , epoch after epoch it should go down , no harm at one or two places\n",
    "# trainng loss goes up then problem\n",
    "# even spikes come then check your model\n",
    "# validation loss coming up then we think it is overfitting\n",
    "# like if we keep epoch =2000 here then overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac751fec-e32a-48d0-8f2a-cad702cd0bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history.keys() # tensor flow output is a dictionary, here: history.history, history is object\n",
    "# .history is an attribute of history "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7b5a67-d6f3-46c4-8e59-7581006fefc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73473a3d-528c-4904-8067-c1cf083ef692",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1334d9c1-b4bc-49fe-bb0a-f1cbbb9206d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history.history  \n",
    "#dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a45e3d0-548a-482f-b034-4ab5a63d81ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_df= pd.DataFrame(history.history)\n",
    "display(hist_df.head())\n",
    "fig, ax = plt.subplots(1,2)\n",
    "\n",
    "# the required plot\n",
    "hist_df.plot(y= ['loss','val_loss'], ax= ax[0])\n",
    "hist_df.plot(y= ['accuracy','val_accuracy'], ax= ax[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86ce9c5-d958-4b15-90ef-7861cd55e87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_plot_tf_hist(hist_df=hist_df) #Using function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60a5c47-b197-4df3-9fa1-f97780059976",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_train)\n",
    "accuracy_score(y_train, y_pred.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8780231e-4869-4a32-b900-caa40a233711",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_train, y_pred.argmax(axis=1))\n",
    "plost = ConfusionMatrixDisplay(cm, display_labels = [0,1])\n",
    "fig, ax = plt.subplots(figsize = (4,4))\n",
    "\n",
    "plost.plot(ax = ax, cmap = 'Blues', colorbar = False)\n",
    "ax.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4274c63f-c3f6-4f10-bf99-18502f7ad613",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "accuracy_score(y_test, y_pred.argmax(axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6573e5-681c-4c84-8353-44174b875dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred.argmax(axis = 1))\n",
    "plat = ConfusionMatrixDisplay(cm, display_labels = [0, 1])\n",
    "fig, ax =plt.subplots(figsize = (4,4))\n",
    "\n",
    "plat.plot(ax = ax, cmap = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
